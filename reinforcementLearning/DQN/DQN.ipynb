{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建环境内物体对象的class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建GridWorld环境对应的class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(), 1, 1, 2, None, 'hero')\n",
    "        self.objects.append(hero)\n",
    "        goal = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal)\n",
    "        hole = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole)\n",
    "        goal2 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal2)\n",
    "        hole2 = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole2)\n",
    "        goal3 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal3)\n",
    "        goal4 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    def moveChar(self, direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        if direction == 0 and hero.y>=1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <=self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >=1:\n",
    "            hero.x -= 1\n",
    "        if direction ==3 and hero.x<=self.sizeX-2:\n",
    "            hero.x += 1\n",
    "        self.objects[0] = hero\n",
    "        \n",
    "    def newPosition(self):\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else :\n",
    "                others.append(obj)\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y ==other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 1, 1, 'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 0, -1, 'fire'))\n",
    "                return  other.reward, False\n",
    "        return 0.0, False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2, self.sizeX+2, 3])\n",
    "        a[1:-1, 1:-1, :] = 0\n",
    "        hero =None\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp=\"nearest\")\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp=\"nearest\")\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp=\"nearest\")\n",
    "        a = np.stack([b,c,d], axis=2)\n",
    "        return a\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward, done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5efe8d72e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32, \n",
    "                                                     kernel_size=[8,8], stride=[4,4], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64, \n",
    "                                                     kernel_size=[4,4], stride=[2,2], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64, \n",
    "                                                     kernel_size=[3,3], stride=[1,1], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=h_size, \n",
    "                                                     kernel_size=[7,7], stride=[1,1], padding=\"VALID\", biases_initializer=None)\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW =  tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        #print(\"predict:\", self.predict)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot=tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现Experience Replay 策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer)+len(experience) >=self.buffer_size:\n",
    "            self.buffer[0:(len(self.buffer)+len(experience)-self.buffer_size)]=[]\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        #print(\"size: \", size)\n",
    "        #print(self.buffer)\n",
    "        array = np.array(random.sample(self.buffer, size))\n",
    "        #print(array)       \n",
    "        \n",
    "        return np.reshape(array, [size, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    #print(\"tfVars: \", type(tfVars))\n",
    "    #print(\"total_vars: \", total_vars)\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    " \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 4\n",
    "y = .99\n",
    "startE =  1\n",
    "endE = 0.1\n",
    "anneling_step = 10000\n",
    "num_episodes = 10000\n",
    "pre_train_steps = 10000\n",
    "max_epLength = 50\n",
    "load_model = False\n",
    "path = \"./model\"\n",
    "h_size = 512\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "trainables = tf.trainable_variables()\n",
    "#print(len(trainables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetOps = updateTargetGraph(trainables, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myBuffer = experience_buffer()\n",
    "e= startE\n",
    "stepDrop = (startE-endE)/anneling_step\n",
    "\n",
    "rList = []\n",
    "total_steps = 0\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 , average reward of last 25 episode  1.88\n",
      "episode 50 , average reward of last 25 episode  2.56\n",
      "episode 75 , average reward of last 25 episode  1.6\n",
      "episode 100 , average reward of last 25 episode  2.48\n",
      "episode 125 , average reward of last 25 episode  2.48\n",
      "episode 150 , average reward of last 25 episode  2.0\n",
      "episode 175 , average reward of last 25 episode  1.76\n",
      "episode 200 , average reward of last 25 episode  2.56\n",
      "episode 225 , average reward of last 25 episode  2.68\n",
      "episode 250 , average reward of last 25 episode  2.16\n",
      "episode 275 , average reward of last 25 episode  0.88\n",
      "episode 300 , average reward of last 25 episode  1.72\n",
      "episode 325 , average reward of last 25 episode  1.52\n",
      "episode 350 , average reward of last 25 episode  0.6\n",
      "episode 375 , average reward of last 25 episode  0.84\n",
      "episode 400 , average reward of last 25 episode  1.16\n",
      "episode 425 , average reward of last 25 episode  0.88\n",
      "episode 450 , average reward of last 25 episode  1.04\n",
      "episode 475 , average reward of last 25 episode  0.64\n",
      "episode 500 , average reward of last 25 episode  1.0\n",
      "episode 525 , average reward of last 25 episode  0.92\n",
      "episode 550 , average reward of last 25 episode  0.72\n",
      "episode 575 , average reward of last 25 episode  0.24\n",
      "episode 600 , average reward of last 25 episode  0.76\n",
      "episode 625 , average reward of last 25 episode  0.64\n",
      "episode 650 , average reward of last 25 episode  0.56\n",
      "episode 675 , average reward of last 25 episode  0.96\n",
      "episode 700 , average reward of last 25 episode  1.12\n",
      "episode 725 , average reward of last 25 episode  0.72\n",
      "episode 750 , average reward of last 25 episode  1.6\n",
      "episode 775 , average reward of last 25 episode  0.84\n",
      "episode 800 , average reward of last 25 episode  1.12\n",
      "episode 825 , average reward of last 25 episode  0.12\n",
      "episode 850 , average reward of last 25 episode  0.64\n",
      "episode 875 , average reward of last 25 episode  1.08\n",
      "episode 900 , average reward of last 25 episode  1.68\n",
      "episode 925 , average reward of last 25 episode  1.24\n",
      "episode 950 , average reward of last 25 episode  0.68\n",
      "episode 975 , average reward of last 25 episode  1.24\n",
      "episode 1000 , average reward of last 25 episode  0.92\n",
      "saved model\n",
      "episode 1025 , average reward of last 25 episode  1.36\n",
      "episode 1050 , average reward of last 25 episode  1.16\n",
      "episode 1075 , average reward of last 25 episode  1.68\n",
      "episode 1100 , average reward of last 25 episode  1.16\n",
      "episode 1125 , average reward of last 25 episode  1.12\n",
      "episode 1150 , average reward of last 25 episode  0.68\n",
      "episode 1175 , average reward of last 25 episode  1.52\n",
      "episode 1200 , average reward of last 25 episode  1.32\n",
      "episode 1225 , average reward of last 25 episode  1.32\n",
      "episode 1250 , average reward of last 25 episode  1.6\n",
      "episode 1275 , average reward of last 25 episode  1.48\n",
      "episode 1300 , average reward of last 25 episode  0.88\n",
      "episode 1325 , average reward of last 25 episode  0.84\n",
      "episode 1350 , average reward of last 25 episode  1.68\n",
      "episode 1375 , average reward of last 25 episode  1.36\n",
      "episode 1400 , average reward of last 25 episode  1.2\n",
      "episode 1425 , average reward of last 25 episode  1.48\n",
      "episode 1450 , average reward of last 25 episode  1.64\n",
      "episode 1475 , average reward of last 25 episode  1.44\n",
      "episode 1500 , average reward of last 25 episode  1.2\n",
      "episode 1525 , average reward of last 25 episode  0.88\n",
      "episode 1550 , average reward of last 25 episode  1.12\n",
      "episode 1575 , average reward of last 25 episode  1.36\n",
      "episode 1600 , average reward of last 25 episode  2.36\n",
      "episode 1625 , average reward of last 25 episode  1.2\n",
      "episode 1650 , average reward of last 25 episode  1.68\n",
      "episode 1675 , average reward of last 25 episode  1.96\n",
      "episode 1700 , average reward of last 25 episode  1.24\n",
      "episode 1725 , average reward of last 25 episode  0.6\n",
      "episode 1750 , average reward of last 25 episode  1.68\n",
      "episode 1775 , average reward of last 25 episode  1.64\n",
      "episode 1800 , average reward of last 25 episode  2.04\n",
      "episode 1825 , average reward of last 25 episode  2.48\n",
      "episode 1850 , average reward of last 25 episode  1.64\n",
      "episode 1875 , average reward of last 25 episode  1.64\n",
      "episode 1900 , average reward of last 25 episode  2.24\n",
      "episode 1925 , average reward of last 25 episode  1.68\n",
      "episode 1950 , average reward of last 25 episode  2.36\n",
      "episode 1975 , average reward of last 25 episode  2.24\n",
      "episode 2000 , average reward of last 25 episode  2.8\n",
      "saved model\n",
      "episode 2025 , average reward of last 25 episode  3.56\n",
      "episode 2050 , average reward of last 25 episode  2.68\n",
      "episode 2075 , average reward of last 25 episode  3.56\n",
      "episode 2100 , average reward of last 25 episode  3.36\n",
      "episode 2125 , average reward of last 25 episode  2.88\n",
      "episode 2150 , average reward of last 25 episode  3.72\n",
      "episode 2175 , average reward of last 25 episode  2.88\n",
      "episode 2200 , average reward of last 25 episode  3.72\n",
      "episode 2225 , average reward of last 25 episode  2.68\n",
      "episode 2250 , average reward of last 25 episode  3.72\n",
      "episode 2275 , average reward of last 25 episode  4.68\n",
      "episode 2300 , average reward of last 25 episode  4.12\n",
      "episode 2325 , average reward of last 25 episode  3.8\n",
      "episode 2350 , average reward of last 25 episode  4.2\n",
      "episode 2375 , average reward of last 25 episode  3.28\n",
      "episode 2400 , average reward of last 25 episode  3.16\n",
      "episode 2425 , average reward of last 25 episode  3.92\n",
      "episode 2450 , average reward of last 25 episode  5.84\n",
      "episode 2475 , average reward of last 25 episode  5.32\n",
      "episode 2500 , average reward of last 25 episode  5.64\n",
      "episode 2525 , average reward of last 25 episode  5.32\n",
      "episode 2550 , average reward of last 25 episode  5.84\n",
      "episode 2575 , average reward of last 25 episode  5.56\n",
      "episode 2600 , average reward of last 25 episode  7.68\n",
      "episode 2625 , average reward of last 25 episode  8.16\n",
      "episode 2650 , average reward of last 25 episode  9.08\n",
      "episode 2675 , average reward of last 25 episode  9.68\n",
      "episode 2700 , average reward of last 25 episode  7.16\n",
      "episode 2725 , average reward of last 25 episode  7.28\n",
      "episode 2750 , average reward of last 25 episode  9.04\n",
      "episode 2775 , average reward of last 25 episode  8.76\n",
      "episode 2800 , average reward of last 25 episode  9.56\n",
      "episode 2825 , average reward of last 25 episode  9.44\n",
      "episode 2850 , average reward of last 25 episode  11.64\n",
      "episode 2875 , average reward of last 25 episode  11.4\n",
      "episode 2900 , average reward of last 25 episode  11.28\n",
      "episode 2925 , average reward of last 25 episode  10.96\n",
      "episode 2950 , average reward of last 25 episode  11.2\n",
      "episode 2975 , average reward of last 25 episode  12.0\n",
      "episode 3000 , average reward of last 25 episode  11.84\n",
      "saved model\n",
      "episode 3025 , average reward of last 25 episode  14.88\n",
      "episode 3050 , average reward of last 25 episode  12.84\n",
      "episode 3075 , average reward of last 25 episode  12.84\n",
      "episode 3100 , average reward of last 25 episode  13.2\n",
      "episode 3125 , average reward of last 25 episode  12.04\n",
      "episode 3150 , average reward of last 25 episode  12.6\n",
      "episode 3175 , average reward of last 25 episode  15.96\n",
      "episode 3200 , average reward of last 25 episode  16.4\n",
      "episode 3225 , average reward of last 25 episode  13.92\n",
      "episode 3250 , average reward of last 25 episode  17.92\n",
      "episode 3275 , average reward of last 25 episode  14.84\n",
      "episode 3300 , average reward of last 25 episode  16.28\n",
      "episode 3325 , average reward of last 25 episode  14.52\n",
      "episode 3350 , average reward of last 25 episode  17.88\n",
      "episode 3375 , average reward of last 25 episode  16.52\n",
      "episode 3400 , average reward of last 25 episode  15.92\n",
      "episode 3425 , average reward of last 25 episode  15.16\n",
      "episode 3450 , average reward of last 25 episode  19.72\n",
      "episode 3475 , average reward of last 25 episode  16.72\n",
      "episode 3500 , average reward of last 25 episode  16.32\n",
      "episode 3525 , average reward of last 25 episode  20.28\n",
      "episode 3550 , average reward of last 25 episode  17.16\n",
      "episode 3575 , average reward of last 25 episode  18.6\n",
      "episode 3600 , average reward of last 25 episode  17.16\n",
      "episode 3625 , average reward of last 25 episode  19.36\n",
      "episode 3650 , average reward of last 25 episode  17.48\n",
      "episode 3675 , average reward of last 25 episode  18.0\n",
      "episode 3700 , average reward of last 25 episode  18.72\n",
      "episode 3725 , average reward of last 25 episode  19.0\n",
      "episode 3750 , average reward of last 25 episode  19.16\n",
      "episode 3775 , average reward of last 25 episode  19.68\n",
      "episode 3800 , average reward of last 25 episode  19.2\n",
      "episode 3825 , average reward of last 25 episode  19.36\n",
      "episode 3850 , average reward of last 25 episode  21.8\n",
      "episode 3875 , average reward of last 25 episode  20.76\n",
      "episode 3900 , average reward of last 25 episode  19.88\n",
      "episode 3925 , average reward of last 25 episode  20.92\n",
      "episode 3950 , average reward of last 25 episode  19.44\n",
      "episode 3975 , average reward of last 25 episode  21.04\n",
      "episode 4000 , average reward of last 25 episode  20.32\n",
      "saved model\n",
      "episode 4025 , average reward of last 25 episode  20.24\n",
      "episode 4050 , average reward of last 25 episode  21.16\n",
      "episode 4075 , average reward of last 25 episode  17.88\n",
      "episode 4100 , average reward of last 25 episode  20.4\n",
      "episode 4125 , average reward of last 25 episode  19.56\n",
      "episode 4150 , average reward of last 25 episode  20.12\n",
      "episode 4175 , average reward of last 25 episode  20.08\n",
      "episode 4200 , average reward of last 25 episode  20.68\n",
      "episode 4225 , average reward of last 25 episode  20.72\n",
      "episode 4250 , average reward of last 25 episode  20.6\n",
      "episode 4275 , average reward of last 25 episode  21.92\n",
      "episode 4300 , average reward of last 25 episode  19.6\n",
      "episode 4325 , average reward of last 25 episode  20.4\n",
      "episode 4350 , average reward of last 25 episode  20.0\n",
      "episode 4375 , average reward of last 25 episode  21.76\n",
      "episode 4400 , average reward of last 25 episode  22.0\n",
      "episode 4425 , average reward of last 25 episode  21.28\n",
      "episode 4450 , average reward of last 25 episode  21.28\n",
      "episode 4475 , average reward of last 25 episode  20.92\n",
      "episode 4500 , average reward of last 25 episode  21.24\n",
      "episode 4525 , average reward of last 25 episode  20.36\n",
      "episode 4550 , average reward of last 25 episode  21.68\n",
      "episode 4575 , average reward of last 25 episode  20.68\n",
      "episode 4600 , average reward of last 25 episode  21.76\n",
      "episode 4625 , average reward of last 25 episode  22.76\n",
      "episode 4650 , average reward of last 25 episode  22.08\n",
      "episode 4675 , average reward of last 25 episode  22.96\n",
      "episode 4700 , average reward of last 25 episode  21.2\n",
      "episode 4725 , average reward of last 25 episode  22.08\n",
      "episode 4750 , average reward of last 25 episode  19.88\n",
      "episode 4775 , average reward of last 25 episode  20.4\n",
      "episode 4800 , average reward of last 25 episode  21.12\n",
      "episode 4825 , average reward of last 25 episode  19.68\n",
      "episode 4850 , average reward of last 25 episode  21.32\n",
      "episode 4875 , average reward of last 25 episode  20.92\n",
      "episode 4900 , average reward of last 25 episode  22.4\n",
      "episode 4925 , average reward of last 25 episode  21.4\n",
      "episode 4950 , average reward of last 25 episode  22.08\n",
      "episode 4975 , average reward of last 25 episode  21.36\n",
      "episode 5000 , average reward of last 25 episode  21.8\n",
      "saved model\n",
      "episode 5025 , average reward of last 25 episode  20.44\n",
      "episode 5050 , average reward of last 25 episode  22.52\n",
      "episode 5075 , average reward of last 25 episode  22.12\n",
      "episode 5100 , average reward of last 25 episode  21.8\n",
      "episode 5125 , average reward of last 25 episode  21.28\n",
      "episode 5150 , average reward of last 25 episode  21.48\n",
      "episode 5175 , average reward of last 25 episode  21.0\n",
      "episode 5200 , average reward of last 25 episode  20.64\n",
      "episode 5225 , average reward of last 25 episode  22.04\n",
      "episode 5250 , average reward of last 25 episode  21.08\n",
      "episode 5275 , average reward of last 25 episode  21.72\n",
      "episode 5300 , average reward of last 25 episode  21.48\n",
      "episode 5325 , average reward of last 25 episode  22.2\n",
      "episode 5350 , average reward of last 25 episode  23.2\n",
      "episode 5375 , average reward of last 25 episode  21.48\n",
      "episode 5400 , average reward of last 25 episode  22.48\n",
      "episode 5425 , average reward of last 25 episode  23.56\n",
      "episode 5450 , average reward of last 25 episode  22.24\n",
      "episode 5475 , average reward of last 25 episode  21.0\n",
      "episode 5500 , average reward of last 25 episode  22.68\n",
      "episode 5525 , average reward of last 25 episode  21.68\n",
      "episode 5550 , average reward of last 25 episode  21.84\n",
      "episode 5575 , average reward of last 25 episode  21.72\n",
      "episode 5600 , average reward of last 25 episode  21.96\n",
      "episode 5625 , average reward of last 25 episode  22.0\n",
      "episode 5650 , average reward of last 25 episode  20.68\n",
      "episode 5675 , average reward of last 25 episode  21.16\n",
      "episode 5700 , average reward of last 25 episode  22.88\n",
      "episode 5725 , average reward of last 25 episode  22.64\n",
      "episode 5750 , average reward of last 25 episode  22.08\n",
      "episode 5775 , average reward of last 25 episode  20.12\n",
      "episode 5800 , average reward of last 25 episode  21.8\n",
      "episode 5825 , average reward of last 25 episode  22.48\n",
      "episode 5850 , average reward of last 25 episode  22.76\n",
      "episode 5875 , average reward of last 25 episode  21.08\n",
      "episode 5900 , average reward of last 25 episode  21.28\n",
      "episode 5925 , average reward of last 25 episode  21.44\n",
      "episode 5950 , average reward of last 25 episode  22.8\n",
      "episode 5975 , average reward of last 25 episode  21.6\n",
      "episode 6000 , average reward of last 25 episode  20.92\n",
      "saved model\n",
      "episode 6025 , average reward of last 25 episode  21.0\n",
      "episode 6050 , average reward of last 25 episode  23.8\n",
      "episode 6075 , average reward of last 25 episode  21.16\n",
      "episode 6100 , average reward of last 25 episode  22.0\n",
      "episode 6125 , average reward of last 25 episode  23.44\n",
      "episode 6150 , average reward of last 25 episode  21.8\n",
      "episode 6175 , average reward of last 25 episode  21.32\n",
      "episode 6200 , average reward of last 25 episode  21.8\n",
      "episode 6225 , average reward of last 25 episode  22.84\n",
      "episode 6250 , average reward of last 25 episode  21.48\n",
      "episode 6275 , average reward of last 25 episode  23.28\n",
      "episode 6300 , average reward of last 25 episode  22.6\n",
      "episode 6325 , average reward of last 25 episode  21.16\n",
      "episode 6350 , average reward of last 25 episode  21.52\n",
      "episode 6375 , average reward of last 25 episode  22.76\n",
      "episode 6400 , average reward of last 25 episode  21.88\n",
      "episode 6425 , average reward of last 25 episode  22.24\n",
      "episode 6450 , average reward of last 25 episode  22.24\n",
      "episode 6475 , average reward of last 25 episode  21.96\n",
      "episode 6500 , average reward of last 25 episode  21.6\n",
      "episode 6525 , average reward of last 25 episode  22.76\n",
      "episode 6550 , average reward of last 25 episode  22.68\n",
      "episode 6575 , average reward of last 25 episode  20.88\n",
      "episode 6600 , average reward of last 25 episode  22.76\n",
      "episode 6625 , average reward of last 25 episode  22.4\n",
      "episode 6650 , average reward of last 25 episode  23.96\n",
      "episode 6675 , average reward of last 25 episode  21.96\n",
      "episode 6700 , average reward of last 25 episode  23.0\n",
      "episode 6725 , average reward of last 25 episode  21.72\n",
      "episode 6750 , average reward of last 25 episode  22.04\n",
      "episode 6775 , average reward of last 25 episode  22.72\n",
      "episode 6800 , average reward of last 25 episode  22.2\n",
      "episode 6825 , average reward of last 25 episode  21.36\n",
      "episode 6850 , average reward of last 25 episode  21.68\n",
      "episode 6875 , average reward of last 25 episode  21.12\n",
      "episode 6900 , average reward of last 25 episode  22.76\n",
      "episode 6925 , average reward of last 25 episode  22.52\n",
      "episode 6950 , average reward of last 25 episode  21.44\n",
      "episode 6975 , average reward of last 25 episode  22.76\n",
      "episode 7000 , average reward of last 25 episode  21.04\n",
      "saved model\n",
      "episode 7025 , average reward of last 25 episode  21.56\n",
      "episode 7050 , average reward of last 25 episode  22.32\n",
      "episode 7075 , average reward of last 25 episode  24.2\n",
      "episode 7100 , average reward of last 25 episode  21.72\n",
      "episode 7125 , average reward of last 25 episode  22.28\n",
      "episode 7150 , average reward of last 25 episode  22.6\n",
      "episode 7175 , average reward of last 25 episode  21.12\n",
      "episode 7200 , average reward of last 25 episode  22.0\n",
      "episode 7225 , average reward of last 25 episode  21.8\n",
      "episode 7250 , average reward of last 25 episode  20.92\n",
      "episode 7275 , average reward of last 25 episode  21.56\n",
      "episode 7300 , average reward of last 25 episode  22.92\n",
      "episode 7325 , average reward of last 25 episode  23.56\n",
      "episode 7350 , average reward of last 25 episode  22.36\n",
      "episode 7375 , average reward of last 25 episode  21.72\n",
      "episode 7400 , average reward of last 25 episode  23.0\n",
      "episode 7425 , average reward of last 25 episode  22.04\n",
      "episode 7450 , average reward of last 25 episode  22.84\n",
      "episode 7475 , average reward of last 25 episode  22.68\n",
      "episode 7500 , average reward of last 25 episode  21.36\n",
      "episode 7525 , average reward of last 25 episode  22.08\n",
      "episode 7550 , average reward of last 25 episode  22.08\n",
      "episode 7575 , average reward of last 25 episode  23.04\n",
      "episode 7600 , average reward of last 25 episode  22.88\n",
      "episode 7625 , average reward of last 25 episode  23.36\n",
      "episode 7650 , average reward of last 25 episode  22.36\n",
      "episode 7675 , average reward of last 25 episode  21.84\n",
      "episode 7700 , average reward of last 25 episode  24.32\n",
      "episode 7725 , average reward of last 25 episode  21.68\n",
      "episode 7750 , average reward of last 25 episode  22.36\n",
      "episode 7775 , average reward of last 25 episode  21.56\n",
      "episode 7800 , average reward of last 25 episode  22.48\n",
      "episode 7825 , average reward of last 25 episode  21.24\n",
      "episode 7850 , average reward of last 25 episode  22.96\n",
      "episode 7875 , average reward of last 25 episode  22.84\n",
      "episode 7900 , average reward of last 25 episode  22.4\n",
      "episode 7925 , average reward of last 25 episode  22.52\n",
      "episode 7950 , average reward of last 25 episode  23.12\n",
      "episode 7975 , average reward of last 25 episode  21.56\n",
      "episode 8000 , average reward of last 25 episode  22.64\n",
      "saved model\n",
      "episode 8025 , average reward of last 25 episode  23.88\n",
      "episode 8050 , average reward of last 25 episode  22.84\n",
      "episode 8075 , average reward of last 25 episode  22.28\n",
      "episode 8100 , average reward of last 25 episode  23.96\n",
      "episode 8125 , average reward of last 25 episode  21.56\n",
      "episode 8150 , average reward of last 25 episode  22.04\n",
      "episode 8175 , average reward of last 25 episode  20.56\n",
      "episode 8200 , average reward of last 25 episode  22.88\n",
      "episode 8225 , average reward of last 25 episode  24.24\n",
      "episode 8250 , average reward of last 25 episode  22.32\n",
      "episode 8275 , average reward of last 25 episode  22.0\n",
      "episode 8300 , average reward of last 25 episode  23.44\n",
      "episode 8325 , average reward of last 25 episode  22.32\n",
      "episode 8350 , average reward of last 25 episode  22.56\n",
      "episode 8375 , average reward of last 25 episode  22.92\n",
      "episode 8400 , average reward of last 25 episode  22.84\n",
      "episode 8425 , average reward of last 25 episode  22.56\n",
      "episode 8450 , average reward of last 25 episode  22.12\n",
      "episode 8475 , average reward of last 25 episode  22.84\n",
      "episode 8500 , average reward of last 25 episode  22.44\n",
      "episode 8525 , average reward of last 25 episode  21.8\n",
      "episode 8550 , average reward of last 25 episode  23.44\n",
      "episode 8575 , average reward of last 25 episode  23.08\n",
      "episode 8600 , average reward of last 25 episode  21.84\n",
      "episode 8625 , average reward of last 25 episode  23.08\n",
      "episode 8650 , average reward of last 25 episode  23.28\n",
      "episode 8675 , average reward of last 25 episode  22.56\n",
      "episode 8700 , average reward of last 25 episode  23.32\n",
      "episode 8725 , average reward of last 25 episode  23.32\n",
      "episode 8750 , average reward of last 25 episode  22.76\n",
      "episode 8775 , average reward of last 25 episode  21.56\n",
      "episode 8800 , average reward of last 25 episode  23.08\n",
      "episode 8825 , average reward of last 25 episode  21.72\n",
      "episode 8850 , average reward of last 25 episode  23.68\n",
      "episode 8875 , average reward of last 25 episode  22.92\n",
      "episode 8900 , average reward of last 25 episode  22.4\n",
      "episode 8925 , average reward of last 25 episode  21.8\n",
      "episode 8950 , average reward of last 25 episode  22.44\n",
      "episode 8975 , average reward of last 25 episode  21.4\n",
      "episode 9000 , average reward of last 25 episode  22.08\n",
      "saved model\n",
      "episode 9025 , average reward of last 25 episode  21.64\n",
      "episode 9050 , average reward of last 25 episode  22.28\n",
      "episode 9075 , average reward of last 25 episode  22.52\n",
      "episode 9100 , average reward of last 25 episode  22.2\n",
      "episode 9125 , average reward of last 25 episode  20.72\n",
      "episode 9150 , average reward of last 25 episode  22.72\n",
      "episode 9175 , average reward of last 25 episode  21.88\n",
      "episode 9200 , average reward of last 25 episode  22.92\n",
      "episode 9225 , average reward of last 25 episode  21.2\n",
      "episode 9250 , average reward of last 25 episode  21.68\n",
      "episode 9275 , average reward of last 25 episode  23.12\n",
      "episode 9300 , average reward of last 25 episode  22.84\n",
      "episode 9325 , average reward of last 25 episode  21.88\n",
      "episode 9350 , average reward of last 25 episode  21.52\n",
      "episode 9375 , average reward of last 25 episode  22.32\n",
      "episode 9400 , average reward of last 25 episode  22.44\n",
      "episode 9425 , average reward of last 25 episode  22.28\n",
      "episode 9450 , average reward of last 25 episode  22.44\n",
      "episode 9475 , average reward of last 25 episode  23.04\n",
      "episode 9500 , average reward of last 25 episode  22.0\n",
      "episode 9525 , average reward of last 25 episode  22.88\n",
      "episode 9550 , average reward of last 25 episode  23.32\n",
      "episode 9575 , average reward of last 25 episode  24.12\n",
      "episode 9600 , average reward of last 25 episode  22.44\n",
      "episode 9625 , average reward of last 25 episode  22.64\n",
      "episode 9650 , average reward of last 25 episode  23.36\n",
      "episode 9675 , average reward of last 25 episode  21.92\n",
      "episode 9700 , average reward of last 25 episode  22.48\n",
      "episode 9725 , average reward of last 25 episode  23.72\n",
      "episode 9750 , average reward of last 25 episode  22.08\n",
      "episode 9775 , average reward of last 25 episode  22.88\n",
      "episode 9800 , average reward of last 25 episode  23.36\n",
      "episode 9825 , average reward of last 25 episode  22.96\n",
      "episode 9850 , average reward of last 25 episode  22.36\n",
      "episode 9875 , average reward of last 25 episode  21.92\n",
      "episode 9900 , average reward of last 25 episode  21.8\n",
      "episode 9925 , average reward of last 25 episode  22.48\n",
      "episode 9950 , average reward of last 25 episode  23.48\n",
      "episode 9975 , average reward of last 25 episode  22.04\n",
      "episode 10000 , average reward of last 25 episode  23.92\n",
      "saved model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "   \n",
    "    updateTarget(targetOps, sess)\n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        while j< max_epLength:\n",
    "            j += 1\n",
    "            if np.random.rand(1) <e or total_steps<pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "                \n",
    "            s1, r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps +=1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]), [1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -=stepDrop\n",
    "                if total_steps%(update_freq)==0 :\n",
    "                \n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    #print(\"trainBatch:\", type(trainBatch),trainBatch.shape, trainBatch[:,4]-1)\n",
    "                    A = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    doubleQ = Q[range(batch_size), A]\n",
    "                    #print(\"A\", type(A),A.shape, A)\n",
    "                    #print(\"Q\", type(Q), A.shape, Q)\n",
    "                    #print(\"doubleQ\",len(doubleQ),  doubleQ)\n",
    "                    end_multiplier = -(trainBatch[:,4] -1)\n",
    "                    targetQ = trainBatch[:,2]+ (y*doubleQ*end_multiplier)\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), \n",
    "                                                            mainQN.targetQ: targetQ,\n",
    "                                                            mainQN.actions:trainBatch[:,1]})\n",
    "                    updateTarget(targetOps, sess)\n",
    "                \n",
    "            rAll +=r\n",
    "            s =s1\n",
    "            if d==True:\n",
    "                break\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "        if i>0 and i%25 ==0:\n",
    "            print(\"episode\", i, \", average reward of last 25 episode \", np.mean(rList[-25:]))\n",
    "        if i>0 and i%1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"saved model\")\n",
    "    saver.save(sess, path+'/model-' + str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ed2259748>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ef4934f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
