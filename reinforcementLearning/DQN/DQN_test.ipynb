{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建环境内物体对象的class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建GridWorld环境对应的class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(), 1, 1, 2, None, 'hero')\n",
    "        self.objects.append(hero)\n",
    "        goal = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal)\n",
    "        hole = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole)\n",
    "        goal2 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal2)\n",
    "        hole2 = gameOb(self.newPosition(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole2)\n",
    "        goal3 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal3)\n",
    "        goal4 = gameOb(self.newPosition(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal4)\n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    def moveChar(self, direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        if direction == 0 and hero.y>=1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <=self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >=1:\n",
    "            hero.x -= 1\n",
    "        if direction ==3 and hero.x<=self.sizeX-2:\n",
    "            hero.x += 1\n",
    "        self.objects[0] = hero\n",
    "        \n",
    "    def newPosition(self):\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else :\n",
    "                others.append(obj)\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y ==other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 1, 1, 'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(), 1, 1, 0, -1, 'fire'))\n",
    "                return  other.reward, False\n",
    "        return 0.0, False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2, self.sizeX+2, 3])\n",
    "        a[1:-1, 1:-1, :] = 0\n",
    "        hero =None\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp=\"nearest\")\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp=\"nearest\")\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp=\"nearest\")\n",
    "        a = np.stack([b,c,d], axis=2)\n",
    "        return a\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward, done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ea61d7828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32, \n",
    "                                                     kernel_size=[8,8], stride=[4,4], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64, \n",
    "                                                     kernel_size=[4,4], stride=[2,2], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64, \n",
    "                                                     kernel_size=[3,3], stride=[1,1], padding=\"VALID\", biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=h_size, \n",
    "                                                     kernel_size=[7,7], stride=[1,1], padding=\"VALID\", biases_initializer=None)\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW =  tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        print(\"self.Qout \", type(self.Qout),self.Qout.shape)\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        print(\"self.predict\", type(self.predict), self.predict.shape)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        print(\"self.targetQ\", type(self.targetQ), self.targetQ.shape)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        print(\"self.actions :\", type(self.actions), self.actions.shape)\n",
    "        self.actions_onehot=tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        print(\"self.actions_onehot\", type(self.actions_onehot), self.actions_onehot.shape)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        print(\"self.Q\", type(self.Q), (self.Q).shape)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现Experience Replay 策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer)+len(experience) >=self.buffer_size:\n",
    "            self.buffer[0:(len(self.buffer)+len(experience)-self.buffer_size)]=[]\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        #print(\"size: \", size)\n",
    "        #print(self.buffer)\n",
    "        array = np.array(random.sample(self.buffer, size))\n",
    "        #print(array)       \n",
    "        \n",
    "        return np.reshape(array, [size, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    #print(\"tfVars: \", type(tfVars))\n",
    "    #print(\"total_vars: \", total_vars)\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    " \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "update_freq = 4\n",
    "y = .99\n",
    "startE =  1\n",
    "endE = 0.1\n",
    "anneling_step = 100\n",
    "num_episodes = 10000\n",
    "pre_train_steps = 1000\n",
    "max_epLength = 50\n",
    "load_model = False\n",
    "path = \"./model\"\n",
    "h_size = 512\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.Qout  <class 'tensorflow.python.framework.ops.Tensor'> (?, 4)\n",
      "self.predict <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.targetQ <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.actions : <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.actions_onehot <class 'tensorflow.python.framework.ops.Tensor'> (?, 4)\n",
      "self.Q <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.Qout  <class 'tensorflow.python.framework.ops.Tensor'> (?, 4)\n",
      "self.predict <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.targetQ <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.actions : <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n",
      "self.actions_onehot <class 'tensorflow.python.framework.ops.Tensor'> (?, 4)\n",
      "self.Q <class 'tensorflow.python.framework.ops.Tensor'> (?,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "trainables = tf.trainable_variables()\n",
    "#print(len(trainables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetOps = updateTargetGraph(trainables, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myBuffer = experience_buffer()\n",
    "e= startE\n",
    "stepDrop = (startE-endE)/anneling_step\n",
    "\n",
    "rList = []\n",
    "total_steps = 0\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 , average reward of last 25 episode  1.64\n",
      "episode 50 , average reward of last 25 episode  1.08\n",
      "episode 75 , average reward of last 25 episode  0.32\n",
      "episode 100 , average reward of last 25 episode  0.48\n",
      "episode 125 , average reward of last 25 episode  0.96\n",
      "episode 150 , average reward of last 25 episode  1.16\n",
      "episode 175 , average reward of last 25 episode  0.4\n",
      "episode 200 , average reward of last 25 episode  0.36\n",
      "episode 225 , average reward of last 25 episode  0.64\n",
      "episode 250 , average reward of last 25 episode  0.4\n",
      "episode 275 , average reward of last 25 episode  0.96\n",
      "episode 300 , average reward of last 25 episode  0.16\n",
      "episode 325 , average reward of last 25 episode  0.84\n",
      "episode 350 , average reward of last 25 episode  0.84\n",
      "episode 375 , average reward of last 25 episode  0.52\n",
      "episode 400 , average reward of last 25 episode  -0.08\n",
      "episode 425 , average reward of last 25 episode  0.6\n",
      "episode 450 , average reward of last 25 episode  0.52\n",
      "episode 475 , average reward of last 25 episode  0.76\n",
      "episode 500 , average reward of last 25 episode  1.28\n",
      "episode 525 , average reward of last 25 episode  1.12\n",
      "episode 550 , average reward of last 25 episode  0.72\n",
      "episode 575 , average reward of last 25 episode  0.64\n",
      "episode 600 , average reward of last 25 episode  1.24\n",
      "episode 625 , average reward of last 25 episode  1.2\n",
      "episode 650 , average reward of last 25 episode  0.8\n",
      "episode 675 , average reward of last 25 episode  1.2\n",
      "episode 700 , average reward of last 25 episode  0.68\n",
      "episode 725 , average reward of last 25 episode  0.84\n",
      "episode 750 , average reward of last 25 episode  1.6\n",
      "episode 775 , average reward of last 25 episode  0.76\n",
      "episode 800 , average reward of last 25 episode  1.32\n",
      "episode 825 , average reward of last 25 episode  0.96\n",
      "episode 850 , average reward of last 25 episode  1.0\n",
      "episode 875 , average reward of last 25 episode  1.24\n",
      "episode 900 , average reward of last 25 episode  1.08\n",
      "episode 925 , average reward of last 25 episode  0.4\n",
      "episode 950 , average reward of last 25 episode  0.68\n",
      "episode 975 , average reward of last 25 episode  1.36\n",
      "episode 1000 , average reward of last 25 episode  0.92\n",
      "saved model\n",
      "episode 1025 , average reward of last 25 episode  1.68\n",
      "episode 1050 , average reward of last 25 episode  1.72\n",
      "episode 1075 , average reward of last 25 episode  1.04\n",
      "episode 1100 , average reward of last 25 episode  1.24\n",
      "episode 1125 , average reward of last 25 episode  1.56\n",
      "episode 1150 , average reward of last 25 episode  1.8\n",
      "episode 1175 , average reward of last 25 episode  1.48\n",
      "episode 1200 , average reward of last 25 episode  1.16\n",
      "episode 1225 , average reward of last 25 episode  1.48\n",
      "episode 1250 , average reward of last 25 episode  1.48\n",
      "episode 1275 , average reward of last 25 episode  2.2\n",
      "episode 1300 , average reward of last 25 episode  2.36\n",
      "episode 1325 , average reward of last 25 episode  1.84\n",
      "episode 1350 , average reward of last 25 episode  2.0\n",
      "episode 1375 , average reward of last 25 episode  1.88\n",
      "episode 1400 , average reward of last 25 episode  1.84\n",
      "episode 1425 , average reward of last 25 episode  1.88\n",
      "episode 1450 , average reward of last 25 episode  2.44\n",
      "episode 1475 , average reward of last 25 episode  2.36\n",
      "episode 1500 , average reward of last 25 episode  2.96\n",
      "episode 1525 , average reward of last 25 episode  2.2\n",
      "episode 1550 , average reward of last 25 episode  3.28\n",
      "episode 1575 , average reward of last 25 episode  3.2\n",
      "episode 1600 , average reward of last 25 episode  2.56\n",
      "episode 1625 , average reward of last 25 episode  3.64\n",
      "episode 1650 , average reward of last 25 episode  3.32\n",
      "episode 1675 , average reward of last 25 episode  7.0\n",
      "episode 1700 , average reward of last 25 episode  4.52\n",
      "episode 1725 , average reward of last 25 episode  6.0\n",
      "episode 1750 , average reward of last 25 episode  4.88\n",
      "episode 1775 , average reward of last 25 episode  5.04\n",
      "episode 1800 , average reward of last 25 episode  6.8\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "   \n",
    "    updateTarget(targetOps, sess)\n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        while j< max_epLength:\n",
    "            j += 1\n",
    "            if np.random.rand(1) <e or total_steps<pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "                \n",
    "            s1, r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps +=1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]), [1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -=stepDrop\n",
    "                if total_steps%(update_freq)==0 :\n",
    "                \n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    #print(\"trainBatch:\", type(trainBatch),trainBatch.shape, trainBatch[:,4]-1)\n",
    "                    A = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    doubleQ = Q[range(batch_size), A]\n",
    "                    #print(\"A\", type(A),A.shape, A)\n",
    "                    #print(\"Q\", type(Q), A.shape, Q)\n",
    "                    #print(\"doubleQ\",len(doubleQ),  doubleQ)\n",
    "                    end_multiplier = -(trainBatch[:,4] -1)\n",
    "                    targetQ = trainBatch[:,2]+ (y*doubleQ*end_multiplier)\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]), \n",
    "                                                            mainQN.targetQ: targetQ,\n",
    "                                                            mainQN.actions:trainBatch[:,1]})\n",
    "                    updateTarget(targetOps, sess)\n",
    "                \n",
    "            rAll +=r\n",
    "            s =s1\n",
    "            if d==True:\n",
    "                break\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "        if i>0 and i%25 ==0:\n",
    "            print(\"episode\", i, \", average reward of last 25 episode \", np.mean(rList[-25:]))\n",
    "        if i>0 and i%1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"saved model\")\n",
    "    saver.save(sess, path+'/model-' + str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ed2259748>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ef4934f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
