{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 利用CNN实现cifar10 的图像识别任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 下载 cifar10 数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 100.0%\n",
      "Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\n"
     ]
    }
   ],
   "source": [
    "import cifar10, cifar10_input\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_steps = 3000\n",
    "batch_size=128\n",
    "data_dir = '/tmp/cifar10_data/cifar-10-batches-bin' # cifar10默认被下载到这个位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练数据和测试数据，其中训练数据使用了数据增强技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 构建CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),  wl, name=\"weight_loss\")\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24,24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "构建层结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一层 卷积层\n",
    "weight1 = variable_with_weight_loss(shape=[5,5,3,64], stddev=5e-2, wl=0.0)\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.conv2d(image_holder, weight1, strides=[1,1,1,1], padding='SAME')\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(conv1, bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "# 第二层 卷积层\n",
    "weight2 = variable_with_weight_loss(shape=[5,5,64,64], stddev=5e-2, wl=0.0)\n",
    "bias2 =  tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.conv2d(norm1, weight2, [1,1,1,1], padding='SAME')\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(conv2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# 第三层 全连接层\n",
    "reshape = tf.reshape(pool2, [batch_size,-1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 =variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)\n",
    "\n",
    "# 第四层  全连接层\n",
    "weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n",
    "\n",
    "# 第五层  输出层\n",
    "weight5 = variable_with_weight_loss(shape=[192,10], stddev=1/192.0, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels,name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    \n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = loss(logits, label_holder)\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 140313924466432)>,\n",
       " <Thread(Thread-5, started daemon 140313916073728)>,\n",
       " <Thread(Thread-6, started daemon 140313762985728)>,\n",
       " <Thread(Thread-7, started daemon 140313754593024)>,\n",
       " <Thread(Thread-8, started daemon 140313746200320)>,\n",
       " <Thread(Thread-9, started daemon 140313737807616)>,\n",
       " <Thread(Thread-10, started daemon 140313729414912)>,\n",
       " <Thread(Thread-11, started daemon 140313721022208)>,\n",
       " <Thread(Thread-12, started daemon 140313712629504)>,\n",
       " <Thread(Thread-13, started daemon 140313427441408)>,\n",
       " <Thread(Thread-14, started daemon 140313419048704)>,\n",
       " <Thread(Thread-15, started daemon 140313410656000)>,\n",
       " <Thread(Thread-16, started daemon 140313402263296)>,\n",
       " <Thread(Thread-17, started daemon 140313393870592)>,\n",
       " <Thread(Thread-18, started daemon 140313385477888)>,\n",
       " <Thread(Thread-19, started daemon 140313377085184)>,\n",
       " <Thread(Thread-20, started daemon 140313368692480)>,\n",
       " <Thread(Thread-21, started daemon 140313360299776)>,\n",
       " <Thread(Thread-22, started daemon 140313351907072)>,\n",
       " <Thread(Thread-23, started daemon 140313343514368)>,\n",
       " <Thread(Thread-24, started daemon 140313335121664)>,\n",
       " <Thread(Thread-25, started daemon 140313326728960)>,\n",
       " <Thread(Thread-26, started daemon 140313318336256)>,\n",
       " <Thread(Thread-27, started daemon 140313309943552)>,\n",
       " <Thread(Thread-28, started daemon 140313293158144)>,\n",
       " <Thread(Thread-29, started daemon 140313284765440)>,\n",
       " <Thread(Thread-30, started daemon 140313276372736)>,\n",
       " <Thread(Thread-31, started daemon 140313267980032)>,\n",
       " <Thread(Thread-32, started daemon 140313259587328)>,\n",
       " <Thread(Thread-33, started daemon 140313251194624)>,\n",
       " <Thread(Thread-34, started daemon 140313242801920)>,\n",
       " <Thread(Thread-35, started daemon 140313234409216)>,\n",
       " <Thread(Thread-36, started daemon 140313226016512)>,\n",
       " <Thread(Thread-37, started daemon 140313217623808)>,\n",
       " <Thread(Thread-38, started daemon 140313209231104)>,\n",
       " <Thread(Thread-39, started daemon 140313200838400)>,\n",
       " <Thread(Thread-40, started daemon 140313192445696)>,\n",
       " <Thread(Thread-41, started daemon 140313184052992)>,\n",
       " <Thread(Thread-42, started daemon 140313175660288)>,\n",
       " <Thread(Thread-43, started daemon 140313167267584)>,\n",
       " <Thread(Thread-44, started daemon 140313158874880)>,\n",
       " <Thread(Thread-45, started daemon 140313150482176)>,\n",
       " <Thread(Thread-46, started daemon 140313142089472)>,\n",
       " <Thread(Thread-47, started daemon 140313133696768)>,\n",
       " <Thread(Thread-48, started daemon 140313125304064)>,\n",
       " <Thread(Thread-49, started daemon 140313116911360)>,\n",
       " <Thread(Thread-50, started daemon 140313108518656)>,\n",
       " <Thread(Thread-51, started daemon 140313100125952)>,\n",
       " <Thread(Thread-52, started daemon 140313091733248)>,\n",
       " <Thread(Thread-53, started daemon 140313083340544)>,\n",
       " <Thread(Thread-54, started daemon 140313074947840)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 7.05 (52.0 examples/sec; 2.462 sec/batch)\n",
      "step 10, loss = 5.14 (71.1 examples/sec; 1.800 sec/batch)\n",
      "step 20, loss = 3.91 (74.5 examples/sec; 1.718 sec/batch)\n",
      "step 30, loss = 3.30 (70.9 examples/sec; 1.805 sec/batch)\n",
      "step 40, loss = 2.74 (71.4 examples/sec; 1.792 sec/batch)\n",
      "step 50, loss = 2.33 (64.0 examples/sec; 1.999 sec/batch)\n",
      "step 60, loss = 2.20 (45.7 examples/sec; 2.801 sec/batch)\n",
      "step 70, loss = 2.13 (67.4 examples/sec; 1.899 sec/batch)\n",
      "step 80, loss = 2.17 (66.8 examples/sec; 1.915 sec/batch)\n",
      "step 90, loss = 1.80 (42.1 examples/sec; 3.037 sec/batch)\n",
      "step 100, loss = 1.95 (77.2 examples/sec; 1.658 sec/batch)\n",
      "step 110, loss = 1.97 (56.0 examples/sec; 2.287 sec/batch)\n",
      "step 120, loss = 1.78 (41.6 examples/sec; 3.081 sec/batch)\n",
      "step 130, loss = 1.91 (58.1 examples/sec; 2.204 sec/batch)\n",
      "step 140, loss = 1.94 (53.6 examples/sec; 2.388 sec/batch)\n",
      "step 150, loss = 1.82 (53.0 examples/sec; 2.413 sec/batch)\n",
      "step 160, loss = 1.88 (64.9 examples/sec; 1.974 sec/batch)\n",
      "step 170, loss = 1.68 (70.0 examples/sec; 1.829 sec/batch)\n",
      "step 180, loss = 1.86 (63.1 examples/sec; 2.029 sec/batch)\n",
      "step 190, loss = 1.69 (56.3 examples/sec; 2.275 sec/batch)\n",
      "step 200, loss = 1.66 (50.2 examples/sec; 2.551 sec/batch)\n",
      "step 210, loss = 1.71 (68.6 examples/sec; 1.867 sec/batch)\n",
      "step 220, loss = 1.77 (71.8 examples/sec; 1.784 sec/batch)\n",
      "step 230, loss = 1.60 (77.7 examples/sec; 1.647 sec/batch)\n",
      "step 240, loss = 1.68 (83.4 examples/sec; 1.534 sec/batch)\n",
      "step 250, loss = 1.65 (82.1 examples/sec; 1.560 sec/batch)\n",
      "step 260, loss = 1.53 (84.9 examples/sec; 1.507 sec/batch)\n",
      "step 270, loss = 1.62 (85.6 examples/sec; 1.496 sec/batch)\n",
      "step 280, loss = 1.54 (87.5 examples/sec; 1.463 sec/batch)\n",
      "step 290, loss = 1.46 (70.4 examples/sec; 1.819 sec/batch)\n",
      "step 300, loss = 1.60 (78.6 examples/sec; 1.629 sec/batch)\n",
      "step 310, loss = 1.46 (70.9 examples/sec; 1.805 sec/batch)\n",
      "step 320, loss = 1.54 (83.0 examples/sec; 1.542 sec/batch)\n",
      "step 330, loss = 1.52 (83.9 examples/sec; 1.526 sec/batch)\n",
      "step 340, loss = 1.72 (84.3 examples/sec; 1.519 sec/batch)\n",
      "step 350, loss = 1.47 (81.3 examples/sec; 1.574 sec/batch)\n",
      "step 360, loss = 1.63 (78.5 examples/sec; 1.630 sec/batch)\n",
      "step 370, loss = 1.47 (83.7 examples/sec; 1.529 sec/batch)\n",
      "step 380, loss = 1.60 (75.3 examples/sec; 1.700 sec/batch)\n",
      "step 390, loss = 1.39 (82.8 examples/sec; 1.547 sec/batch)\n",
      "step 400, loss = 1.49 (81.4 examples/sec; 1.573 sec/batch)\n",
      "step 410, loss = 1.38 (81.4 examples/sec; 1.572 sec/batch)\n",
      "step 420, loss = 1.50 (81.1 examples/sec; 1.579 sec/batch)\n",
      "step 430, loss = 1.53 (79.5 examples/sec; 1.609 sec/batch)\n",
      "step 440, loss = 1.36 (71.9 examples/sec; 1.780 sec/batch)\n",
      "step 450, loss = 1.46 (86.3 examples/sec; 1.483 sec/batch)\n",
      "step 460, loss = 1.62 (66.8 examples/sec; 1.917 sec/batch)\n",
      "step 470, loss = 1.50 (79.6 examples/sec; 1.609 sec/batch)\n",
      "step 480, loss = 1.49 (75.5 examples/sec; 1.695 sec/batch)\n",
      "step 490, loss = 1.64 (82.6 examples/sec; 1.550 sec/batch)\n",
      "step 500, loss = 1.47 (79.2 examples/sec; 1.616 sec/batch)\n",
      "step 510, loss = 1.46 (64.5 examples/sec; 1.984 sec/batch)\n",
      "step 520, loss = 1.52 (64.2 examples/sec; 1.994 sec/batch)\n",
      "step 530, loss = 1.49 (83.5 examples/sec; 1.533 sec/batch)\n",
      "step 540, loss = 1.64 (80.3 examples/sec; 1.595 sec/batch)\n",
      "step 550, loss = 1.34 (84.1 examples/sec; 1.522 sec/batch)\n",
      "step 560, loss = 1.36 (88.2 examples/sec; 1.452 sec/batch)\n",
      "step 570, loss = 1.45 (79.8 examples/sec; 1.604 sec/batch)\n",
      "step 580, loss = 1.46 (82.4 examples/sec; 1.554 sec/batch)\n",
      "step 590, loss = 1.41 (85.6 examples/sec; 1.496 sec/batch)\n",
      "step 600, loss = 1.49 (84.1 examples/sec; 1.522 sec/batch)\n",
      "step 610, loss = 1.48 (84.7 examples/sec; 1.512 sec/batch)\n",
      "step 620, loss = 1.51 (86.6 examples/sec; 1.478 sec/batch)\n",
      "step 630, loss = 1.40 (82.1 examples/sec; 1.559 sec/batch)\n",
      "step 640, loss = 1.47 (84.8 examples/sec; 1.509 sec/batch)\n",
      "step 650, loss = 1.46 (77.0 examples/sec; 1.662 sec/batch)\n",
      "step 660, loss = 1.23 (76.7 examples/sec; 1.669 sec/batch)\n",
      "step 670, loss = 1.14 (79.8 examples/sec; 1.604 sec/batch)\n",
      "step 680, loss = 1.52 (85.4 examples/sec; 1.499 sec/batch)\n",
      "step 690, loss = 1.40 (71.4 examples/sec; 1.792 sec/batch)\n",
      "step 700, loss = 1.47 (83.5 examples/sec; 1.534 sec/batch)\n",
      "step 710, loss = 1.30 (84.0 examples/sec; 1.523 sec/batch)\n",
      "step 720, loss = 1.38 (85.4 examples/sec; 1.499 sec/batch)\n",
      "step 730, loss = 1.26 (84.5 examples/sec; 1.515 sec/batch)\n",
      "step 740, loss = 1.24 (82.4 examples/sec; 1.553 sec/batch)\n",
      "step 750, loss = 1.17 (80.9 examples/sec; 1.583 sec/batch)\n",
      "step 760, loss = 1.42 (80.9 examples/sec; 1.583 sec/batch)\n",
      "step 770, loss = 1.41 (86.3 examples/sec; 1.483 sec/batch)\n",
      "step 780, loss = 1.45 (84.1 examples/sec; 1.522 sec/batch)\n",
      "step 790, loss = 1.20 (79.3 examples/sec; 1.614 sec/batch)\n",
      "step 800, loss = 1.29 (78.9 examples/sec; 1.623 sec/batch)\n",
      "step 810, loss = 1.45 (78.5 examples/sec; 1.630 sec/batch)\n",
      "step 820, loss = 1.39 (81.2 examples/sec; 1.576 sec/batch)\n",
      "step 830, loss = 1.29 (82.3 examples/sec; 1.556 sec/batch)\n",
      "step 840, loss = 1.24 (60.8 examples/sec; 2.105 sec/batch)\n",
      "step 850, loss = 1.43 (82.0 examples/sec; 1.561 sec/batch)\n",
      "step 860, loss = 1.47 (80.4 examples/sec; 1.591 sec/batch)\n",
      "step 870, loss = 1.29 (80.6 examples/sec; 1.588 sec/batch)\n",
      "step 880, loss = 1.26 (65.8 examples/sec; 1.945 sec/batch)\n",
      "step 890, loss = 1.50 (83.4 examples/sec; 1.535 sec/batch)\n",
      "step 900, loss = 1.11 (79.0 examples/sec; 1.620 sec/batch)\n",
      "step 910, loss = 1.45 (82.4 examples/sec; 1.553 sec/batch)\n",
      "step 920, loss = 1.26 (80.3 examples/sec; 1.594 sec/batch)\n",
      "step 930, loss = 1.39 (78.9 examples/sec; 1.622 sec/batch)\n",
      "step 940, loss = 1.39 (75.9 examples/sec; 1.687 sec/batch)\n",
      "step 950, loss = 1.38 (85.9 examples/sec; 1.490 sec/batch)\n",
      "step 960, loss = 1.21 (75.8 examples/sec; 1.689 sec/batch)\n",
      "step 970, loss = 1.44 (85.2 examples/sec; 1.503 sec/batch)\n",
      "step 980, loss = 1.37 (77.2 examples/sec; 1.659 sec/batch)\n",
      "step 990, loss = 1.18 (75.7 examples/sec; 1.691 sec/batch)\n",
      "step 1000, loss = 1.27 (78.7 examples/sec; 1.627 sec/batch)\n",
      "step 1010, loss = 1.25 (87.1 examples/sec; 1.469 sec/batch)\n",
      "step 1020, loss = 1.28 (86.3 examples/sec; 1.483 sec/batch)\n",
      "step 1030, loss = 1.48 (67.2 examples/sec; 1.905 sec/batch)\n",
      "step 1040, loss = 1.30 (79.4 examples/sec; 1.613 sec/batch)\n",
      "step 1050, loss = 1.20 (86.2 examples/sec; 1.485 sec/batch)\n",
      "step 1060, loss = 1.43 (76.0 examples/sec; 1.685 sec/batch)\n",
      "step 1070, loss = 1.21 (79.2 examples/sec; 1.616 sec/batch)\n",
      "step 1080, loss = 1.34 (83.1 examples/sec; 1.541 sec/batch)\n",
      "step 1090, loss = 1.40 (76.1 examples/sec; 1.682 sec/batch)\n",
      "step 1100, loss = 1.13 (61.8 examples/sec; 2.071 sec/batch)\n",
      "step 1110, loss = 1.13 (87.0 examples/sec; 1.471 sec/batch)\n",
      "step 1120, loss = 1.41 (76.8 examples/sec; 1.666 sec/batch)\n",
      "step 1130, loss = 1.30 (78.4 examples/sec; 1.632 sec/batch)\n",
      "step 1140, loss = 1.22 (73.1 examples/sec; 1.752 sec/batch)\n",
      "step 1150, loss = 1.12 (87.9 examples/sec; 1.456 sec/batch)\n",
      "step 1160, loss = 1.27 (87.0 examples/sec; 1.471 sec/batch)\n",
      "step 1170, loss = 1.27 (82.1 examples/sec; 1.559 sec/batch)\n",
      "step 1180, loss = 1.34 (83.7 examples/sec; 1.529 sec/batch)\n",
      "step 1190, loss = 1.10 (84.2 examples/sec; 1.521 sec/batch)\n",
      "step 1200, loss = 1.29 (70.1 examples/sec; 1.827 sec/batch)\n",
      "step 1210, loss = 1.38 (88.4 examples/sec; 1.447 sec/batch)\n",
      "step 1220, loss = 1.07 (82.9 examples/sec; 1.545 sec/batch)\n",
      "step 1230, loss = 1.22 (85.3 examples/sec; 1.500 sec/batch)\n",
      "step 1240, loss = 1.34 (74.2 examples/sec; 1.725 sec/batch)\n",
      "step 1250, loss = 1.20 (87.5 examples/sec; 1.463 sec/batch)\n",
      "step 1260, loss = 1.15 (84.8 examples/sec; 1.509 sec/batch)\n",
      "step 1270, loss = 1.30 (87.1 examples/sec; 1.469 sec/batch)\n",
      "step 1280, loss = 1.38 (82.1 examples/sec; 1.560 sec/batch)\n",
      "step 1290, loss = 1.20 (85.8 examples/sec; 1.492 sec/batch)\n",
      "step 1300, loss = 1.14 (67.0 examples/sec; 1.910 sec/batch)\n",
      "step 1310, loss = 1.20 (84.2 examples/sec; 1.519 sec/batch)\n",
      "step 1320, loss = 1.24 (73.0 examples/sec; 1.754 sec/batch)\n",
      "step 1330, loss = 1.39 (85.7 examples/sec; 1.494 sec/batch)\n",
      "step 1340, loss = 1.15 (81.6 examples/sec; 1.569 sec/batch)\n",
      "step 1350, loss = 1.27 (87.5 examples/sec; 1.464 sec/batch)\n",
      "step 1360, loss = 1.20 (85.6 examples/sec; 1.496 sec/batch)\n",
      "step 1370, loss = 1.07 (87.6 examples/sec; 1.462 sec/batch)\n",
      "step 1380, loss = 1.26 (87.6 examples/sec; 1.461 sec/batch)\n",
      "step 1390, loss = 1.20 (54.0 examples/sec; 2.372 sec/batch)\n",
      "step 1400, loss = 1.25 (67.7 examples/sec; 1.891 sec/batch)\n",
      "step 1410, loss = 1.44 (82.6 examples/sec; 1.550 sec/batch)\n",
      "step 1420, loss = 1.34 (79.3 examples/sec; 1.614 sec/batch)\n",
      "step 1430, loss = 1.27 (85.3 examples/sec; 1.500 sec/batch)\n",
      "step 1440, loss = 1.23 (79.8 examples/sec; 1.603 sec/batch)\n",
      "step 1450, loss = 1.23 (83.2 examples/sec; 1.538 sec/batch)\n",
      "step 1460, loss = 1.01 (74.1 examples/sec; 1.729 sec/batch)\n",
      "step 1470, loss = 1.19 (77.9 examples/sec; 1.643 sec/batch)\n",
      "step 1480, loss = 1.07 (75.9 examples/sec; 1.686 sec/batch)\n",
      "step 1490, loss = 1.16 (79.4 examples/sec; 1.612 sec/batch)\n",
      "step 1500, loss = 1.15 (79.7 examples/sec; 1.606 sec/batch)\n",
      "step 1510, loss = 1.17 (82.5 examples/sec; 1.551 sec/batch)\n",
      "step 1520, loss = 1.14 (61.0 examples/sec; 2.097 sec/batch)\n",
      "step 1530, loss = 1.37 (57.0 examples/sec; 2.245 sec/batch)\n",
      "step 1540, loss = 1.30 (82.5 examples/sec; 1.551 sec/batch)\n",
      "step 1550, loss = 1.20 (80.3 examples/sec; 1.594 sec/batch)\n",
      "step 1560, loss = 1.07 (80.2 examples/sec; 1.597 sec/batch)\n",
      "step 1570, loss = 1.25 (77.2 examples/sec; 1.659 sec/batch)\n",
      "step 1580, loss = 1.26 (79.9 examples/sec; 1.602 sec/batch)\n",
      "step 1590, loss = 1.15 (83.0 examples/sec; 1.542 sec/batch)\n",
      "step 1600, loss = 1.24 (76.0 examples/sec; 1.684 sec/batch)\n",
      "step 1610, loss = 1.23 (77.0 examples/sec; 1.663 sec/batch)\n",
      "step 1620, loss = 1.15 (68.3 examples/sec; 1.874 sec/batch)\n",
      "step 1630, loss = 1.25 (83.2 examples/sec; 1.538 sec/batch)\n",
      "step 1640, loss = 1.14 (80.8 examples/sec; 1.585 sec/batch)\n",
      "step 1650, loss = 1.39 (85.4 examples/sec; 1.498 sec/batch)\n",
      "step 1660, loss = 1.13 (79.4 examples/sec; 1.612 sec/batch)\n",
      "step 1670, loss = 1.30 (85.8 examples/sec; 1.491 sec/batch)\n",
      "step 1680, loss = 1.22 (82.1 examples/sec; 1.559 sec/batch)\n",
      "step 1690, loss = 1.11 (77.0 examples/sec; 1.663 sec/batch)\n",
      "step 1700, loss = 1.27 (88.2 examples/sec; 1.451 sec/batch)\n",
      "step 1710, loss = 1.31 (78.7 examples/sec; 1.626 sec/batch)\n",
      "step 1720, loss = 1.17 (85.4 examples/sec; 1.500 sec/batch)\n",
      "step 1730, loss = 1.15 (45.0 examples/sec; 2.846 sec/batch)\n",
      "step 1740, loss = 1.21 (79.9 examples/sec; 1.603 sec/batch)\n",
      "step 1750, loss = 1.07 (46.1 examples/sec; 2.775 sec/batch)\n",
      "step 1760, loss = 1.20 (76.1 examples/sec; 1.681 sec/batch)\n",
      "step 1770, loss = 1.36 (76.2 examples/sec; 1.681 sec/batch)\n",
      "step 1780, loss = 1.03 (64.2 examples/sec; 1.994 sec/batch)\n",
      "step 1790, loss = 1.15 (85.0 examples/sec; 1.507 sec/batch)\n",
      "step 1800, loss = 1.24 (69.7 examples/sec; 1.836 sec/batch)\n",
      "step 1810, loss = 1.22 (79.9 examples/sec; 1.602 sec/batch)\n",
      "step 1820, loss = 1.19 (70.6 examples/sec; 1.814 sec/batch)\n",
      "step 1830, loss = 1.28 (79.7 examples/sec; 1.605 sec/batch)\n",
      "step 1840, loss = 1.22 (77.3 examples/sec; 1.656 sec/batch)\n",
      "step 1850, loss = 1.13 (79.3 examples/sec; 1.615 sec/batch)\n",
      "step 1860, loss = 0.96 (86.0 examples/sec; 1.488 sec/batch)\n",
      "step 1870, loss = 1.24 (80.4 examples/sec; 1.592 sec/batch)\n",
      "step 1880, loss = 1.02 (80.7 examples/sec; 1.586 sec/batch)\n",
      "step 1890, loss = 1.23 (83.2 examples/sec; 1.538 sec/batch)\n",
      "step 1900, loss = 1.03 (76.7 examples/sec; 1.668 sec/batch)\n",
      "step 1910, loss = 1.10 (79.2 examples/sec; 1.616 sec/batch)\n",
      "step 1920, loss = 1.36 (78.0 examples/sec; 1.642 sec/batch)\n",
      "step 1930, loss = 1.19 (113.4 examples/sec; 1.128 sec/batch)\n",
      "step 1940, loss = 1.18 (116.2 examples/sec; 1.102 sec/batch)\n",
      "step 1950, loss = 1.08 (103.5 examples/sec; 1.237 sec/batch)\n",
      "step 1960, loss = 0.95 (112.8 examples/sec; 1.135 sec/batch)\n",
      "step 1970, loss = 1.04 (120.9 examples/sec; 1.059 sec/batch)\n",
      "step 1980, loss = 1.26 (113.8 examples/sec; 1.125 sec/batch)\n",
      "step 1990, loss = 1.01 (111.0 examples/sec; 1.153 sec/batch)\n",
      "step 2000, loss = 1.04 (113.1 examples/sec; 1.132 sec/batch)\n",
      "step 2010, loss = 1.09 (109.2 examples/sec; 1.173 sec/batch)\n",
      "step 2020, loss = 1.01 (96.0 examples/sec; 1.334 sec/batch)\n",
      "step 2030, loss = 0.93 (113.6 examples/sec; 1.127 sec/batch)\n",
      "step 2040, loss = 1.27 (116.8 examples/sec; 1.096 sec/batch)\n",
      "step 2050, loss = 1.16 (109.8 examples/sec; 1.166 sec/batch)\n",
      "step 2060, loss = 1.13 (107.1 examples/sec; 1.195 sec/batch)\n",
      "step 2070, loss = 1.04 (112.4 examples/sec; 1.139 sec/batch)\n",
      "step 2080, loss = 1.18 (118.6 examples/sec; 1.080 sec/batch)\n",
      "step 2090, loss = 0.97 (114.3 examples/sec; 1.120 sec/batch)\n",
      "step 2100, loss = 1.20 (109.2 examples/sec; 1.172 sec/batch)\n",
      "step 2110, loss = 1.13 (116.0 examples/sec; 1.104 sec/batch)\n",
      "step 2120, loss = 1.02 (113.4 examples/sec; 1.129 sec/batch)\n",
      "step 2130, loss = 1.19 (105.3 examples/sec; 1.215 sec/batch)\n",
      "step 2140, loss = 1.02 (110.2 examples/sec; 1.161 sec/batch)\n",
      "step 2150, loss = 1.13 (117.2 examples/sec; 1.093 sec/batch)\n",
      "step 2160, loss = 1.10 (57.5 examples/sec; 2.228 sec/batch)\n",
      "step 2170, loss = 1.33 (89.9 examples/sec; 1.424 sec/batch)\n",
      "step 2180, loss = 1.01 (91.0 examples/sec; 1.407 sec/batch)\n",
      "step 2190, loss = 1.03 (89.4 examples/sec; 1.431 sec/batch)\n",
      "step 2200, loss = 1.13 (108.3 examples/sec; 1.182 sec/batch)\n",
      "step 2210, loss = 1.08 (101.4 examples/sec; 1.262 sec/batch)\n",
      "step 2220, loss = 1.25 (110.9 examples/sec; 1.155 sec/batch)\n",
      "step 2230, loss = 1.13 (93.4 examples/sec; 1.371 sec/batch)\n",
      "step 2240, loss = 0.99 (84.2 examples/sec; 1.520 sec/batch)\n",
      "step 2250, loss = 1.10 (98.7 examples/sec; 1.297 sec/batch)\n",
      "step 2260, loss = 1.18 (100.7 examples/sec; 1.271 sec/batch)\n",
      "step 2270, loss = 1.03 (101.4 examples/sec; 1.262 sec/batch)\n",
      "step 2280, loss = 1.10 (66.1 examples/sec; 1.936 sec/batch)\n",
      "step 2290, loss = 1.20 (108.8 examples/sec; 1.176 sec/batch)\n",
      "step 2300, loss = 1.11 (83.2 examples/sec; 1.539 sec/batch)\n",
      "step 2310, loss = 1.13 (118.0 examples/sec; 1.085 sec/batch)\n",
      "step 2320, loss = 1.10 (111.7 examples/sec; 1.146 sec/batch)\n",
      "step 2330, loss = 1.03 (104.8 examples/sec; 1.222 sec/batch)\n",
      "step 2340, loss = 1.13 (112.6 examples/sec; 1.137 sec/batch)\n",
      "step 2350, loss = 1.28 (114.1 examples/sec; 1.121 sec/batch)\n",
      "step 2360, loss = 1.09 (109.2 examples/sec; 1.172 sec/batch)\n",
      "step 2370, loss = 0.91 (106.6 examples/sec; 1.200 sec/batch)\n",
      "step 2380, loss = 1.03 (118.1 examples/sec; 1.084 sec/batch)\n",
      "step 2390, loss = 1.15 (114.5 examples/sec; 1.118 sec/batch)\n",
      "step 2400, loss = 0.98 (106.2 examples/sec; 1.206 sec/batch)\n",
      "step 2410, loss = 1.07 (110.9 examples/sec; 1.154 sec/batch)\n",
      "step 2420, loss = 0.99 (94.9 examples/sec; 1.349 sec/batch)\n",
      "step 2430, loss = 1.11 (110.6 examples/sec; 1.157 sec/batch)\n",
      "step 2440, loss = 1.08 (108.9 examples/sec; 1.175 sec/batch)\n",
      "step 2450, loss = 1.16 (103.2 examples/sec; 1.240 sec/batch)\n",
      "step 2460, loss = 1.12 (112.3 examples/sec; 1.140 sec/batch)\n",
      "step 2470, loss = 1.10 (87.8 examples/sec; 1.457 sec/batch)\n",
      "step 2480, loss = 0.96 (109.2 examples/sec; 1.172 sec/batch)\n",
      "step 2490, loss = 1.00 (105.2 examples/sec; 1.217 sec/batch)\n",
      "step 2500, loss = 1.14 (106.2 examples/sec; 1.205 sec/batch)\n",
      "step 2510, loss = 1.13 (104.6 examples/sec; 1.224 sec/batch)\n",
      "step 2520, loss = 1.07 (109.1 examples/sec; 1.173 sec/batch)\n",
      "step 2530, loss = 1.14 (100.1 examples/sec; 1.279 sec/batch)\n",
      "step 2540, loss = 1.10 (104.1 examples/sec; 1.230 sec/batch)\n",
      "step 2550, loss = 1.17 (107.8 examples/sec; 1.187 sec/batch)\n",
      "step 2560, loss = 1.14 (108.1 examples/sec; 1.184 sec/batch)\n",
      "step 2570, loss = 1.10 (99.3 examples/sec; 1.289 sec/batch)\n",
      "step 2580, loss = 1.03 (107.5 examples/sec; 1.190 sec/batch)\n",
      "step 2590, loss = 1.14 (109.7 examples/sec; 1.167 sec/batch)\n",
      "step 2600, loss = 1.08 (102.9 examples/sec; 1.244 sec/batch)\n",
      "step 2610, loss = 1.15 (107.0 examples/sec; 1.196 sec/batch)\n",
      "step 2620, loss = 0.99 (91.6 examples/sec; 1.397 sec/batch)\n",
      "step 2630, loss = 1.15 (97.7 examples/sec; 1.310 sec/batch)\n",
      "step 2640, loss = 1.12 (107.4 examples/sec; 1.192 sec/batch)\n",
      "step 2650, loss = 0.96 (93.5 examples/sec; 1.369 sec/batch)\n",
      "step 2660, loss = 1.08 (62.9 examples/sec; 2.036 sec/batch)\n",
      "step 2670, loss = 1.24 (96.6 examples/sec; 1.325 sec/batch)\n",
      "step 2680, loss = 1.03 (104.6 examples/sec; 1.224 sec/batch)\n",
      "step 2690, loss = 1.03 (103.7 examples/sec; 1.235 sec/batch)\n",
      "step 2700, loss = 1.10 (102.6 examples/sec; 1.247 sec/batch)\n",
      "step 2710, loss = 1.02 (104.1 examples/sec; 1.230 sec/batch)\n",
      "step 2720, loss = 0.96 (103.9 examples/sec; 1.232 sec/batch)\n",
      "step 2730, loss = 1.22 (104.1 examples/sec; 1.229 sec/batch)\n",
      "step 2740, loss = 0.98 (95.0 examples/sec; 1.347 sec/batch)\n",
      "step 2750, loss = 1.23 (102.0 examples/sec; 1.255 sec/batch)\n",
      "step 2760, loss = 1.16 (99.3 examples/sec; 1.289 sec/batch)\n",
      "step 2770, loss = 1.24 (108.1 examples/sec; 1.184 sec/batch)\n",
      "step 2780, loss = 1.04 (102.1 examples/sec; 1.253 sec/batch)\n",
      "step 2790, loss = 0.94 (96.7 examples/sec; 1.324 sec/batch)\n",
      "step 2800, loss = 1.14 (101.1 examples/sec; 1.266 sec/batch)\n",
      "step 2810, loss = 0.95 (95.5 examples/sec; 1.340 sec/batch)\n",
      "step 2820, loss = 1.00 (100.6 examples/sec; 1.273 sec/batch)\n",
      "step 2830, loss = 1.14 (101.7 examples/sec; 1.259 sec/batch)\n",
      "step 2840, loss = 1.01 (101.1 examples/sec; 1.266 sec/batch)\n",
      "step 2850, loss = 1.20 (89.5 examples/sec; 1.430 sec/batch)\n",
      "step 2860, loss = 1.05 (102.4 examples/sec; 1.250 sec/batch)\n",
      "step 2870, loss = 1.13 (88.2 examples/sec; 1.451 sec/batch)\n",
      "step 2880, loss = 1.07 (100.8 examples/sec; 1.270 sec/batch)\n",
      "step 2890, loss = 1.09 (92.5 examples/sec; 1.383 sec/batch)\n",
      "step 2900, loss = 1.07 (96.9 examples/sec; 1.321 sec/batch)\n",
      "step 2910, loss = 1.11 (102.6 examples/sec; 1.248 sec/batch)\n",
      "step 2920, loss = 0.97 (98.6 examples/sec; 1.298 sec/batch)\n",
      "step 2930, loss = 1.13 (92.9 examples/sec; 1.379 sec/batch)\n",
      "step 2940, loss = 1.23 (100.4 examples/sec; 1.275 sec/batch)\n",
      "step 2950, loss = 1.07 (97.2 examples/sec; 1.317 sec/batch)\n",
      "step 2960, loss = 1.12 (107.5 examples/sec; 1.191 sec/batch)\n",
      "step 2970, loss = 1.18 (94.6 examples/sec; 1.353 sec/batch)\n",
      "step 2980, loss = 1.05 (104.1 examples/sec; 1.229 sec/batch)\n",
      "step 2990, loss = 0.99 (103.5 examples/sec; 1.237 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    image_batch,label_batch =  sess.run([images_train, labels_train])\n",
    "    _, loss_val = sess.run([train_op, loss], feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        examples_per_sec = batch_size/duration\n",
    "        sec_per_batch = float(duration)\n",
    "        format_str = ('step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str%(step, loss_val, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples/batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter*batch_size\n",
    "step = 0\n",
    "while step <num_iter:\n",
    "    image_batch, label_batch = sess.run([images_testst, labels_test])\n",
    "    predictions =sess.run([top_k_op], feed_dictimage_holderolder:image_balabel_holderl_holder:label_batch})\n",
    "    true_count +=%precision.sum(predictions)\n",
    "    step += 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @1=0.711\n"
     ]
    }
   ],
   "source": [
    "precision = true_count/total_sample_count\n",
    "print('precision @1=%.3f'%precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
