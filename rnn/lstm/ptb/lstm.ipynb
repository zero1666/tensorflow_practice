{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. 下载PTB数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载PTB数据集并解压，要确保解压后文件的路径要和接下来python的执行路径一致\n",
    "```\n",
    "wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "tar xvf simple-examples.tgz\n",
    "```\n",
    "\n",
    "解压后下载tensorflow的models库，后面要使用苦衷的PTB reader读取数据内容\n",
    "```\n",
    "git clone https://github.com/tensorflow/models.git\n",
    "cd models/tutorials/rnn/ptb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义处理输入数据的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    def __init__(self,config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data)//batch_size)-1)//num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义语言模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, config, input_):\n",
    "        self._input = input_\n",
    "        batch_size = input_.batch_size\n",
    "        num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "        \n",
    "        attn_cell = lstm_cell\n",
    "        if is_training and config.keep_prob<1:\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "            \n",
    "        if is_training and config.keep_prob<1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "        \n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step >0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "                \n",
    "            output = tf.reshape(tf.concat(outputs, 1), [-1, size])\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "            logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            \n",
    "            loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(input_.targets, [-1])],\n",
    "                                                                                [tf.ones([batch_size*num_steps], dtype=tf.float32)])\n",
    "            self._cost = cost = tf.reduce_sum(loss)/batch_size\n",
    "            self._final_state = state\n",
    "            \n",
    "            if not is_training:\n",
    "                return\n",
    "            \n",
    "            self._lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "            self._train_op = optimizer.apply_gradients(zip(grads, tvars), \n",
    "                                                      global_step= tf.contrib.framework.get_or_create_global_step())\n",
    "            self._new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "            self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "            \n",
    "        def assign_lr(self,session, lr_value):\n",
    "            session.run(self._lr_update, feed_dict={self._new_lr:lr_value})\n",
    "            \n",
    "        @property\n",
    "        def input(self):\n",
    "            return self._input\n",
    "        \n",
    "        @property\n",
    "        def initial_state(self):\n",
    "            return self.initial_state\n",
    "        \n",
    "        @property\n",
    "        def cost(self):\n",
    "            return self._cost\n",
    "        \n",
    "        @proerpty\n",
    "        def final_state(self):\n",
    "            return self._final_state\n",
    "        \n",
    "        @property\n",
    "        def lr(self):\n",
    "            self._lr\n",
    "        \n",
    "        @property\n",
    "        def train_op(self):\n",
    "            return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MediumConfig(object):\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LargeConfig(object):\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1/1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None,  verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    \n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    \n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] =eval_op\n",
    "        \n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict ={}\n",
    "        for i,(c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "            \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "        \n",
    "        if verbose and step%(model.input.epoch_size//10) ==10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\"%\n",
    "                  (step*1.0/model.input.epoch_size, np.exp(costs/iters), iters*model.input.batch_size/(time.time()-start_time)))\n",
    "    return np.exp(costs/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = reader.ptb_raw_data('../simple-examples/data/')\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "config = SmallConfig()\n",
    "eval_config =SmallConfig()\n",
    "eval_config.vatch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PTBModel' object has no attribute '_initial_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2a15e2edf46a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TrainInput\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d03f1b6f0d3e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, config, input_)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PTBModel' object has no attribute '_initial_state'"
     ]
    }
   ],
   "source": [
    "g= tf.Graph()\n",
    "with g.as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse =None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "            \n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid =PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "            \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\",reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv = tf.train.Supervisor()\n",
    "with sv.managed_session() as session:\n",
    "    for i in range(config.max_max_epoch):\n",
    "        lr_decay = config.lr_decay ** max(i+1-config.max_epoch, 0.0)\n",
    "        m.assign_lr(session, config.learning_rate *lr_decay)\n",
    "        \n",
    "        print(\"Epoch: %d Learning rate: %.3f\"%(i+1, session.run(m.lr)))\n",
    "        train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "        print(\"Epoch: %d Train perplexity: %.3f\"%(i+1, train_perplexity()))\n",
    "        valid_perplexity = run_epoch(session, mvalid)\n",
    "        print(\"Epoch: %d Valid perplexity: %.3f\"%(i+1, valid_perplexity()))\n",
    "        \n",
    "    test_perplexity = run_epoch(session, mtest)\n",
    "    print(\"Test Perplexity: %.3f\"%test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
